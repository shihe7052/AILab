{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# å‡è¨­ä½ çš„æª”æ¡ˆåœ¨ /MyDrive/datasets/celeba.zip\n",
        "!unzip \"/content/drive/MyDrive/img_align_celeba.zip\" -d \"/content/datasets/celeba\"\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hEgjcqy6ZCBH",
        "outputId": "7e4677ae-c0fa-446c-863d-543ac0acd43e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Archive:  /content/drive/MyDrive/img_align_celeba.zip\n",
            "checkdir:  cannot create extraction directory: /content/datasets/celeba\n",
            "           No such file or directory\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ==== 0. åŸºæœ¬è¨­å®šï¼ˆæ›è¼‰é›²ç«¯ã€è§£å£“ã€è³‡æ–™é›†è·¯å¾‘ï¼‰ ====\n",
        "import os\n",
        "from google.colab import drive\n",
        "\n",
        "# æ›è¼‰é›²ç«¯\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "ZIP_PATH = \"/content/drive/MyDrive/img_align_celeba.zip\"  # ä½ çš„ zip æª”è·¯å¾‘\n",
        "EXTRACT_DIR = \"/content/datasets/celeba/img_align_celeba\" # è§£å£“å¾Œåœ–ç‰‡æ‰€åœ¨è³‡æ–™å¤¾\n",
        "SAMPLES_DIR = \"/content/drive/MyDrive/celeba_gan_samples\" # ç”Ÿæˆçµæœè¼¸å‡ºè³‡æ–™å¤¾\n",
        "os.makedirs(SAMPLES_DIR, exist_ok=True)\n",
        "\n",
        "# è§£å£“ï¼ˆè‹¥å°šæœªè§£å£“æ‰æœƒåŸ·è¡Œï¼‰\n",
        "if not os.path.exists(EXTRACT_DIR):\n",
        "    os.makedirs(os.path.dirname(EXTRACT_DIR), exist_ok=True)\n",
        "    !unzip -q \"{ZIP_PATH}\" -d \"/content/datasets/celeba\"\n",
        "else:\n",
        "    print(\"ğŸš€ å·²æ‰¾åˆ°è§£å£“å¾Œè³‡æ–™å¤¾ï¼Œç•¥éè§£å£“ã€‚\")\n",
        "\n",
        "# æª¢æŸ¥å‰å¹¾å€‹æª”æ¡ˆ\n",
        "!ls -l \"/content/datasets/celeba\" | head -n 20\n",
        "!ls -l \"{EXTRACT_DIR}\" | head -n 10\n",
        "\n",
        "# ==== 1. åŒ¯å…¥å¥—ä»¶ ====\n",
        "import glob\n",
        "from PIL import Image\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import transforms, utils\n",
        "from torchvision.utils import save_image\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "# ==== 2. è‡ªè¨‚å¹³é¢å½±åƒè³‡æ–™é›†ï¼ˆç„¡é¡åˆ¥è³‡æ–™å¤¾ä¹Ÿå¯è®€ï¼‰ ====\n",
        "class FlatImageFolder(Dataset):\n",
        "    def __init__(self, root, transform=None, exts=(\"jpg\",\"jpeg\",\"png\",\"bmp\",\"webp\")):\n",
        "        self.root = root\n",
        "        self.transform = transform\n",
        "        files = []\n",
        "        for ext in exts:\n",
        "            files.extend(glob.glob(os.path.join(root, f\"**/*.{ext}\"), recursive=True))\n",
        "            files.extend(glob.glob(os.path.join(root, f\"*.{ext}\")))\n",
        "        self.files = sorted(list(set(files)))\n",
        "        if len(self.files) == 0:\n",
        "            raise RuntimeError(f\"No images found under: {root}\")\n",
        "        print(f\"ğŸ–¼ï¸  æ‰¾åˆ° {len(self.files)} å¼µå½±åƒï¼ˆæ–¼ {root}ï¼‰\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.files)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        fp = self.files[idx]\n",
        "        img = Image.open(fp).convert(\"RGB\")\n",
        "        if self.transform:\n",
        "            img = self.transform(img)\n",
        "        # é€™è£¡ä¸éœ€è¦ labelï¼Œå›å‚³ dummy 0\n",
        "        return img, 0\n",
        "\n",
        "# ==== 3. åƒæ•¸èˆ‡è³‡æ–™è¼‰å…¥ ====\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"ğŸ–¥ï¸  Device:\", device)\n",
        "\n",
        "image_size = 64\n",
        "batch_size = 128\n",
        "nz = 100     # latent dim\n",
        "ngf = 64     # G base channels\n",
        "ndf = 64     # D base channels\n",
        "epochs = 5   # ç¤ºç¯„\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize(image_size),\n",
        "    transforms.CenterCrop(image_size),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.5]*3, [0.5]*3),\n",
        "])\n",
        "\n",
        "dataset = FlatImageFolder(EXTRACT_DIR, transform=transform)\n",
        "dataloader = DataLoader(\n",
        "    dataset, batch_size=batch_size, shuffle=True,\n",
        "    num_workers=2, pin_memory=(device.type==\"cuda\")\n",
        ")\n",
        "\n",
        "# ==== 4. DCGAN æ¨¡å‹ï¼ˆD ç„¡ Sigmoidï¼Œé… BCEWithLogitsLossï¼‰ ====\n",
        "class Generator(nn.Module):\n",
        "    def __init__(self, nz=100, ngf=64, nc=3):\n",
        "        super().__init__()\n",
        "        self.main = nn.Sequential(\n",
        "            nn.ConvTranspose2d(nz, ngf*8, 4, 1, 0, bias=False),  # 1x1 -> 4x4\n",
        "            nn.BatchNorm2d(ngf*8),\n",
        "            nn.ReLU(True),\n",
        "\n",
        "            nn.ConvTranspose2d(ngf*8, ngf*4, 4, 2, 1, bias=False),  # 4x4 -> 8x8\n",
        "            nn.BatchNorm2d(ngf*4),\n",
        "            nn.ReLU(True),\n",
        "\n",
        "            nn.ConvTranspose2d(ngf*4, ngf*2, 4, 2, 1, bias=False),  # 8x8 -> 16x16\n",
        "            nn.BatchNorm2d(ngf*2),\n",
        "            nn.ReLU(True),\n",
        "\n",
        "            nn.ConvTranspose2d(ngf*2, ngf, 4, 2, 1, bias=False),    # 16x16 -> 32x32\n",
        "            nn.BatchNorm2d(ngf),\n",
        "            nn.ReLU(True),\n",
        "\n",
        "            nn.ConvTranspose2d(ngf, nc, 4, 2, 1, bias=False),       # 32x32 -> 64x64\n",
        "            nn.Tanh()\n",
        "        )\n",
        "    def forward(self, z):\n",
        "        return self.main(z)\n",
        "\n",
        "class Discriminator(nn.Module):\n",
        "    def __init__(self, ndf=64, nc=3):\n",
        "        super().__init__()\n",
        "        self.main = nn.Sequential(\n",
        "            nn.Conv2d(nc, ndf, 4, 2, 1, bias=False),      # 64->32\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "\n",
        "            nn.Conv2d(ndf, ndf*2, 4, 2, 1, bias=False),   # 32->16\n",
        "            nn.BatchNorm2d(ndf*2),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "\n",
        "            nn.Conv2d(ndf*2, ndf*4, 4, 2, 1, bias=False), # 16->8\n",
        "            nn.BatchNorm2d(ndf*4),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "\n",
        "            nn.Conv2d(ndf*4, ndf*8, 4, 2, 1, bias=False), # 8->4\n",
        "            nn.BatchNorm2d(ndf*8),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "\n",
        "            nn.Conv2d(ndf*8, 1, 4, 1, 0, bias=False)      # 4->1\n",
        "            # ç„¡ Sigmoid\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        return self.main(x).view(-1)\n",
        "\n",
        "netG = Generator(nz, ngf).to(device)\n",
        "netD = Discriminator(ndf).to(device)\n",
        "\n",
        "# æ¬Šé‡åˆå§‹åŒ–ï¼ˆDCGAN å»ºè­° N(0, 0.02)ï¼‰\n",
        "def weights_init(m):\n",
        "    cname = m.__class__.__name__\n",
        "    if cname.find('Conv') != -1 or cname.find('Linear') != -1:\n",
        "        nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
        "    if cname.find('BatchNorm') != -1:\n",
        "        nn.init.normal_(m.weight.data, 1.0, 0.02)\n",
        "        nn.init.zeros_(m.bias.data)\n",
        "\n",
        "netG.apply(weights_init)\n",
        "netD.apply(weights_init)\n",
        "\n",
        "criterion = nn.BCEWithLogitsLoss()\n",
        "optimizerD = optim.Adam(netD.parameters(), lr=2e-4, betas=(0.5, 0.999))\n",
        "optimizerG = optim.Adam(netG.parameters(), lr=2e-4, betas=(0.5, 0.999))\n",
        "\n",
        "# å›ºå®šå™ªè²ï¼Œè¿½è¹¤è¨“ç·´é€²åº¦\n",
        "fixed_noise = torch.randn(64, nz, 1, 1, device=device)\n",
        "\n",
        "# ==== 5. è¨“ç·´ ====\n",
        "netG.train(); netD.train()\n",
        "\n",
        "for epoch in range(1, epochs+1):\n",
        "    for i, (real, _) in enumerate(dataloader):\n",
        "        real = real.to(device)\n",
        "        bsz = real.size(0)\n",
        "\n",
        "        # æ¨™ç±¤ï¼ˆå¯æ¡ label smoothingï¼šçœŸå¯¦=0.9ï¼‰\n",
        "        real_labels = torch.full((bsz,), 0.9, device=device)\n",
        "        fake_labels = torch.zeros(bsz, device=device)\n",
        "\n",
        "        # ---- æ›´æ–° D ----\n",
        "        optimizerD.zero_grad()\n",
        "        pred_real = netD(real)\n",
        "        lossD_real = criterion(pred_real, real_labels)\n",
        "\n",
        "        noise = torch.randn(bsz, nz, 1, 1, device=device)\n",
        "        fake = netG(noise).detach()\n",
        "        pred_fake = netD(fake)\n",
        "        lossD_fake = criterion(pred_fake, fake_labels)\n",
        "\n",
        "        lossD = lossD_real + lossD_fake\n",
        "        lossD.backward()\n",
        "        optimizerD.step()\n",
        "\n",
        "        # ---- æ›´æ–° G ----\n",
        "        optimizerG.zero_grad()\n",
        "        noise = torch.randn(bsz, nz, 1, 1, device=device)\n",
        "        gen = netG(noise)\n",
        "        pred = netD(gen)\n",
        "        lossG = criterion(pred, torch.ones(bsz, device=device))\n",
        "        lossG.backward()\n",
        "        optimizerG.step()\n",
        "\n",
        "    print(f\"[Epoch {epoch}/{epochs}]  LossD: {lossD.item():.4f}  LossG: {lossG.item():.4f}\")\n",
        "\n",
        "    # ä»¥å›ºå®šå™ªè²åŒ¯å‡ºè§€å¯Ÿåœ–\n",
        "    netG.eval()\n",
        "    with torch.no_grad():\n",
        "        samples = netG(fixed_noise).cpu()\n",
        "        save_path = os.path.join(SAMPLES_DIR, f\"epoch_{epoch:03d}.png\")\n",
        "        save_image(samples, save_path, normalize=True, value_range=(-1, 1), nrow=8)\n",
        "        print(\"ğŸ’¾ Saved:\", save_path)\n",
        "    netG.train()\n",
        "\n",
        "    # ï¼ˆå¯é¸ï¼‰å„²å­˜ ckpt\n",
        "    torch.save({\n",
        "        \"G\": netG.state_dict(),\n",
        "        \"D\": netD.state_dict(),\n",
        "        \"optG\": optimizerG.state_dict(),\n",
        "        \"optD\": optimizerD.state_dict(),\n",
        "        \"epoch\": epoch\n",
        "    }, os.path.join(SAMPLES_DIR, f\"ckpt_{epoch:03d}.pt\"))\n",
        "\n",
        "print(\"âœ… è¨“ç·´å®Œæˆï¼æª”æ¡ˆå·²è¼¸å‡ºåˆ°ï¼š\", SAMPLES_DIR)\n",
        "\n",
        "# ==== 6. ç”¢ç”Ÿä¸€æ‰¹æ–°çš„äººè‡‰æ¨£æœ¬ï¼ˆæ”¶å°¾ï¼‰ ====\n",
        "netG.eval()\n",
        "with torch.no_grad():\n",
        "    z = torch.randn(64, nz, 1, 1, device=device)\n",
        "    gen_samples = netG(z).cpu()\n",
        "    save_image(gen_samples, os.path.join(SAMPLES_DIR, \"final_samples.png\"),\n",
        "               normalize=True, value_range=(-1, 1), nrow=8)\n",
        "print(\"ğŸ“· å¦å­˜ä¸€æ‰¹æœ€çµ‚æ¨£æœ¬ï¼šfinal_samples.png\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OsV2lWXMbyOf",
        "outputId": "522f319b-4033-4ac9-df07-cd1242011ae8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "total 5772\n",
            "drwxr-xr-x 2 root root 5906432 Sep 28  2015 img_align_celeba\n",
            "total 1737936\n",
            "-rw-r--r-- 1 root root 11440 Sep 28  2015 000001.jpg\n",
            "-rw-r--r-- 1 root root  7448 Sep 28  2015 000002.jpg\n",
            "-rw-r--r-- 1 root root  4253 Sep 28  2015 000003.jpg\n",
            "-rw-r--r-- 1 root root 10747 Sep 28  2015 000004.jpg\n",
            "-rw-r--r-- 1 root root  6351 Sep 28  2015 000005.jpg\n",
            "-rw-r--r-- 1 root root  8073 Sep 28  2015 000006.jpg\n",
            "-rw-r--r-- 1 root root  8203 Sep 28  2015 000007.jpg\n",
            "-rw-r--r-- 1 root root  7725 Sep 28  2015 000008.jpg\n",
            "-rw-r--r-- 1 root root  8641 Sep 28  2015 000009.jpg\n",
            "ğŸ–¥ï¸  Device: cuda\n",
            "ğŸ–¼ï¸  æ‰¾åˆ° 202599 å¼µå½±åƒï¼ˆæ–¼ /content/datasets/celeba/img_align_celebaï¼‰\n",
            "[Epoch 1/5]  LossD: 0.6953  LossG: 4.3539\n",
            "ğŸ’¾ Saved: /content/drive/MyDrive/celeba_gan_samples/epoch_001.png\n",
            "[Epoch 2/5]  LossD: 1.2970  LossG: 1.0415\n",
            "ğŸ’¾ Saved: /content/drive/MyDrive/celeba_gan_samples/epoch_002.png\n",
            "[Epoch 3/5]  LossD: 0.8110  LossG: 2.1476\n",
            "ğŸ’¾ Saved: /content/drive/MyDrive/celeba_gan_samples/epoch_003.png\n",
            "[Epoch 4/5]  LossD: 1.0071  LossG: 2.9464\n",
            "ğŸ’¾ Saved: /content/drive/MyDrive/celeba_gan_samples/epoch_004.png\n",
            "[Epoch 5/5]  LossD: 1.4452  LossG: 3.6145\n",
            "ğŸ’¾ Saved: /content/drive/MyDrive/celeba_gan_samples/epoch_005.png\n",
            "âœ… è¨“ç·´å®Œæˆï¼æª”æ¡ˆå·²è¼¸å‡ºåˆ°ï¼š /content/drive/MyDrive/celeba_gan_samples\n",
            "ğŸ“· å¦å­˜ä¸€æ‰¹æœ€çµ‚æ¨£æœ¬ï¼šfinal_samples.png\n"
          ]
        }
      ]
    }
  ]
}